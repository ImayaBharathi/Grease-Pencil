{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dalle Model Traininng and testing",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W7HG-Lx9Jjb",
        "outputId": "6b51e391-7e4d-4a4c-84b4-8e73fd72de7b"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ae/79374d2b875e638090600eaa2a423479865b7590c53fb78e8ccf6a64acb1/wandb-0.10.22-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 5.9MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.9MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 51.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 41.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (5.3.1)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=125c867c7467db713a8f349ee941807e62fb173e3d55fcf5a73fc8f60a6d2e44\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=52ae034ef3791fccb55a17d09e37d1d645a22d4ac8816a382eacab0dfdfcc3fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: subprocess32, docker-pycreds, smmap, gitdb, GitPython, sentry-sdk, shortuuid, pathtools, configparser, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKjiMOnlAsgE",
        "outputId": "7f9a7f01-7217-4a13-94b9-efb7657f75bb"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgR2kLuFBYs4",
        "outputId": "7c20526c-eb9a-463c-8c37-7f2ac76bd1a0"
      },
      "source": [
        "import os\r\n",
        "os.listdir(\"/gdrive/My Drive/DALLE\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dataset_for_dalle_2nd_training.zip',\n",
              " 'dataset_middle_three_images_tr.zip',\n",
              " 'dalle_model']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yxVBNBa2gGI"
      },
      "source": [
        "## download the zip file from the below link and upload it into session storage\r\n",
        "## https://drive.google.com/file/d/19Bhlmz4WHPMHMBvyNqsPqX3O7Jyl1DoA/view?usp=sharing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c64uL9YR-eaT"
      },
      "source": [
        "!unzip \"dataset_middle_three_images_tr.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-jbRiN9CMDR",
        "outputId": "042d38bb-4223-4967-a71f-f69b417ff908"
      },
      "source": [
        "len(os.listdir(\"dataset_middle_three_images_tr\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10254"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV-6GPq5Cisr"
      },
      "source": [
        "data_folder_location = \"dataset_middle_three_images_tr\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L15fA3nSEV_T"
      },
      "source": [
        "import torch\r\n",
        "torch.cuda.empty_cache()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrFuBoq3AA8D",
        "outputId": "cd842f7d-e3d7-41b0-c06a-7cc155d8894b"
      },
      "source": [
        "from random import choice\n",
        "from pathlib import Path\n",
        "\n",
        "# torch\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# vision imports\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "# dalle related classes and utils\n",
        "\n",
        "from dalle_pytorch import OpenAIDiscreteVAE, DiscreteVAE, DALLE\n",
        "from dalle_pytorch.simple_tokenizer import tokenize, tokenizer, VOCAB_SIZE\n",
        "\n",
        "# helpers\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "# argument parsing\n",
        "\n",
        "VAE_PATH = None # './vae.pt' - will use OpenAIs pretrained VAE if not set\n",
        "DALLE_PATH = None # './dalle.pt'\n",
        "IMAGE_TEXT_FOLDER = data_folder_location\n",
        "RESUME = exists(DALLE_PATH)\n",
        "\n",
        "# constants\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 5\n",
        "LEARNING_RATE = 3e-4\n",
        "GRAD_CLIP_NORM = 0.5\n",
        "\n",
        "MODEL_DIM = 512\n",
        "TEXT_SEQ_LEN = 256\n",
        "DEPTH = 2\n",
        "HEADS = 4\n",
        "DIM_HEAD = 64\n",
        "\n",
        "# reconstitute vae\n",
        "\n",
        "if RESUME:\n",
        "    dalle_path = Path(DALLE_PATH)\n",
        "    assert dalle_path.exists(), 'DALL-E model file does not exist'\n",
        "\n",
        "    loaded_obj = torch.load(str(dalle_path))\n",
        "\n",
        "    dalle_params, vae_params, weights = loaded_obj['hparams'], loaded_obj['vae_params'], loaded_obj['weights']\n",
        "    vae_params = OpenAIDiscreteVAE()\n",
        "    # vae = DiscreteVAE(**vae_params)\n",
        "\n",
        "    dalle_params = dict(\n",
        "        vae = vae,\n",
        "        **dalle_params\n",
        "    )\n",
        "\n",
        "    IMAGE_SIZE = vae_params['image_size']\n",
        "else:\n",
        "    if exists(VAE_PATH):\n",
        "        vae_path = Path(VAE_PATH)\n",
        "        assert vae_path.exists(), 'VAE model file does not exist'\n",
        "\n",
        "        loaded_obj = torch.load(str(vae_path))\n",
        "\n",
        "        vae_params, weights = loaded_obj['hparams'], loaded_obj['weights']\n",
        "\n",
        "        vae = DiscreteVAE(**vae_params)\n",
        "        vae.load_state_dict(weights)\n",
        "    else:\n",
        "        print('using OpenAIs pretrained VAE for encoding images to tokens')\n",
        "        vae_params = None\n",
        "\n",
        "        vae = OpenAIDiscreteVAE()\n",
        "\n",
        "    IMAGE_SIZE = vae.image_size\n",
        "\n",
        "    dalle_params = dict(\n",
        "        num_text_tokens = VOCAB_SIZE,\n",
        "        text_seq_len = TEXT_SEQ_LEN,\n",
        "        dim = MODEL_DIM,\n",
        "        depth = DEPTH,\n",
        "        heads = HEADS,\n",
        "        dim_head = DIM_HEAD\n",
        "    )\n",
        "\n",
        "# helpers\n",
        "\n",
        "def save_model(path):\n",
        "    save_obj = {\n",
        "        'hparams': dalle_params,\n",
        "        'vae_params': vae_params,\n",
        "        'weights': dalle.state_dict()\n",
        "    }\n",
        "\n",
        "    torch.save(save_obj, path)\n",
        "\n",
        "# dataset loading\n",
        "\n",
        "class TextImageDataset(Dataset):\n",
        "    def __init__(self, folder, text_len = 256, image_size = 128):\n",
        "        super().__init__()\n",
        "        path = Path(folder)\n",
        "\n",
        "        text_files = [*path.glob('**/*.txt')]\n",
        "\n",
        "        image_files = [\n",
        "            *path.glob('**/*.png'),\n",
        "            *path.glob('**/*.jpg'),\n",
        "            *path.glob('**/*.jpeg')\n",
        "        ]\n",
        "\n",
        "        text_files = {t.stem: t for t in text_files}\n",
        "        image_files = {i.stem: i for i in image_files}\n",
        "\n",
        "        keys = (image_files.keys() & text_files.keys())\n",
        "\n",
        "        self.keys = list(keys)\n",
        "        self.text_files = {k: v for k, v in text_files.items() if k in keys}\n",
        "        self.image_files = {k: v for k, v in image_files.items() if k in keys}\n",
        "\n",
        "        self.image_tranform = T.Compose([\n",
        "            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "            T.CenterCrop(image_size),\n",
        "            T.Resize(image_size),\n",
        "            T.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        key = self.keys[ind]\n",
        "        text_file = self.text_files[key]\n",
        "        image_file = self.image_files[key]\n",
        "\n",
        "        image = Image.open(image_file)\n",
        "        descriptions = text_file.read_text().split('\\n')\n",
        "        descriptions = list(filter(lambda t: len(t) > 0, descriptions))\n",
        "        description = choice(descriptions)\n",
        "\n",
        "        tokenized_text = tokenize(description).squeeze(0)\n",
        "        mask = tokenized_text != 0\n",
        "\n",
        "        image_tensor = self.image_tranform(image)\n",
        "        return tokenized_text, image_tensor, mask\n",
        "\n",
        "# create dataset and dataloader\n",
        "\n",
        "ds = TextImageDataset(\n",
        "    IMAGE_TEXT_FOLDER,\n",
        "    text_len = TEXT_SEQ_LEN,\n",
        "    image_size = IMAGE_SIZE\n",
        ")\n",
        "\n",
        "assert len(ds) > 0, 'dataset is empty'\n",
        "print(f'{len(ds)} image-text pairs found for training')\n",
        "\n",
        "dl = DataLoader(ds, batch_size = BATCH_SIZE, shuffle = True, drop_last = True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using OpenAIs pretrained VAE for encoding images to tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|███████████████████████| 215185363/215185363 [00:03<00:00, 64215417.40it/s]\n",
            "100%|███████████████████████| 175360231/175360231 [00:02<00:00, 62358381.65it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5127 image-text pairs found for training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHMfgk4AYaHB"
      },
      "source": [
        "\r\n",
        "# initialize DALL-E\r\n",
        "\r\n",
        "dalle = DALLE(vae = vae, **dalle_params).cuda()\r\n",
        "\r\n",
        "if RESUME:\r\n",
        "    dalle.load_state_dict(weights)\r\n",
        "\r\n",
        "# optimizer\r\n",
        "\r\n",
        "opt = Adam(dalle.parameters(), lr = LEARNING_RATE)\r\n",
        "\r\n",
        "# experiment tracker\r\n",
        "\r\n",
        "import wandb\r\n",
        "\r\n",
        "wandb.config.depth = DEPTH\r\n",
        "wandb.config.heads = HEADS\r\n",
        "wandb.config.dim_head = DIM_HEAD\r\n",
        "\r\n",
        "wandb.init(project = 'dalle_train_transformer_100_images_20_epoch_2desc', resume = RESUME)\r\n",
        "\r\n",
        "# training\r\n",
        "\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "    for i, (text, images, mask) in enumerate(dl):\r\n",
        "        text, images, mask = map(lambda t: t.cuda(), (text, images, mask))\r\n",
        "\r\n",
        "        loss = dalle(text, images, mask = mask, return_loss = True)\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        clip_grad_norm_(dalle.parameters(), GRAD_CLIP_NORM)\r\n",
        "\r\n",
        "        opt.step()\r\n",
        "        opt.zero_grad()\r\n",
        "\r\n",
        "        log = {}\r\n",
        "\r\n",
        "        if i % 10 == 0:\r\n",
        "            print(epoch, i, f'loss - {loss.item()}')\r\n",
        "\r\n",
        "            log = {\r\n",
        "                **log,\r\n",
        "                'epoch': epoch,\r\n",
        "                'iter': i,\r\n",
        "                'loss': loss.item()\r\n",
        "            }\r\n",
        "\r\n",
        "        if i % 100 == 0:\r\n",
        "            sample_text = text[:1]\r\n",
        "            token_list = sample_text.masked_select(sample_text != 0).tolist()\r\n",
        "            decoded_text = tokenizer.decode(token_list)\r\n",
        "\r\n",
        "            image = dalle.generate_images(\r\n",
        "                text[:1],\r\n",
        "                mask = mask[:1],\r\n",
        "                filter_thres = 0.9    # topk sampling at 0.9\r\n",
        "            )\r\n",
        "\r\n",
        "            save_model(f'./dalle.pt')\r\n",
        "            wandb.save(f'./dalle.pt')\r\n",
        "\r\n",
        "            log = {\r\n",
        "                **log,\r\n",
        "                'image': wandb.Image(image, caption = decoded_text)\r\n",
        "            }\r\n",
        "\r\n",
        "        wandb.log(log)\r\n",
        "\r\n",
        "save_model(f'./dalle-final.pt')\r\n",
        "wandb.save('./dalle-final.pt')\r\n",
        "wandb.finish()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx3S2xhL3HGc"
      },
      "source": [
        "### After training run the below file for generating the image from the trained model\r\n",
        "model_path= \"\" ####### Specify the model path here"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub3FzrL1nBiI",
        "outputId": "a506c350-c8e1-4efe-d623-5e7b0d186d02"
      },
      "source": [
        "import argparse\r\n",
        "from pathlib import Path\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "# torch\r\n",
        "\r\n",
        "import torch\r\n",
        "\r\n",
        "from einops import repeat\r\n",
        "\r\n",
        "# vision imports\r\n",
        "\r\n",
        "from PIL import Image\r\n",
        "from torchvision.utils import make_grid, save_image\r\n",
        "\r\n",
        "# dalle related classes and utils\r\n",
        "\r\n",
        "from dalle_pytorch import DiscreteVAE, OpenAIDiscreteVAE, VQGanVAE1024, DALLE\r\n",
        "from dalle_pytorch.simple_tokenizer import tokenize, tokenizer, VOCAB_SIZE\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# load DALL-E\r\n",
        "\r\n",
        "dalle_path = Path(model_path)\r\n",
        "\r\n",
        "assert dalle_path.exists(), 'trained DALL-E must exist'\r\n",
        "\r\n",
        "load_obj = torch.load(str(dalle_path))\r\n",
        "dalle_params, vae_params, weights = load_obj.pop('hparams'), load_obj.pop('vae_params'), load_obj.pop('weights')\r\n",
        "\r\n",
        "dalle_params.pop('vae', None) # cleanup later\r\n",
        "\r\n",
        "# if vae_params is not None:\r\n",
        "#     vae = DiscreteVAE(**vae_params)\r\n",
        "# elif not args.taming:\r\n",
        "vae = OpenAIDiscreteVAE()\r\n",
        "\r\n",
        "\r\n",
        "dalle = DALLE(vae = vae, **dalle_params).cuda()\r\n",
        "\r\n",
        "dalle.load_state_dict(weights)\r\n",
        "\r\n",
        "# generate images\r\n",
        "\r\n",
        "image_size = vae.image_size\r\n",
        "\r\n",
        "text = tokenize([\"a cup of coffee\"], dalle.text_seq_len).cuda()\r\n",
        "\r\n",
        "text = repeat(text, '() n -> b n', b = 128)\r\n",
        "\r\n",
        "outputs = []\r\n",
        "\r\n",
        "for text_chunk in tqdm(text.split(5), desc = 'generating images'):\r\n",
        "    output = dalle.generate_images(text_chunk, filter_thres = 0.9)\r\n",
        "    outputs.append(output)\r\n",
        "\r\n",
        "outputs = torch.cat(outputs)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|███████████████████████| 215185363/215185363 [00:06<00:00, 32433374.38it/s]\n",
            "100%|███████████████████████| 175360231/175360231 [00:09<00:00, 18546900.04it/s]\n",
            "generating images:  96%|█████████▌| 25/26 [21:54<00:52, 52.07s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B8VgXbqvdex"
      },
      "source": [
        "os.mkdir(\"outputs\")\r\n",
        "# save all images\r\n",
        "\r\n",
        "outputs_dir = Path(\"./outputs\")\r\n",
        "outputs_dir.mkdir(parents = True, exist_ok = True)\r\n",
        "\r\n",
        "for i, image in tqdm(enumerate(outputs), desc = 'saving images'):\r\n",
        "    save_image(image, outputs_dir / f'{i}.jpg')\r\n",
        "\r\n",
        "print(f'created 128 images at \"{str(outputs_dir)}\"')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}